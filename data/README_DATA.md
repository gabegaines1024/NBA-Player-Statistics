# Data Files Information

The `data/` directory contains NBA player statistics in JSON format. These files are **ignored by git** (.gitignore) and will be automatically created when you run the pipeline.

## Data Files

### `data/raw.json`
- Raw game log data from NBA API
- Unprocessed, as received from the API
- Contains all original columns and values
- Generated by: `load_or_collect_data()` in `main.py`

### `data/processed.json`
- Cleaned and preprocessed data
- Duplicates removed, outliers handled
- Missing values filled
- Standardized column names
- Generated by: `preprocess_data()` in `main.py`

### `data/feature.json`
- Feature-engineered data ready for ML
- Rolling statistics (3, 5, 10 game windows)
- Efficiency metrics (TS%, eFG%, etc.)
- Time-based features (day of week, rest days, etc.)
- Matchup features (home/away)
- Target variable for next-game prediction
- Generated by: `create_features()` in `main.py`

## Models Directory

The `models/` directory contains trained ML models. These are also **ignored by git**.

### Model Files
- Format: `{model_type}_{target}_{timestamp}.pkl`
- Examples:
  - `random_forest_PTS_20241212_103000.pkl`
  - `gradient_boosting_AST_20241212_110000.pkl`
  - `best_model_PTS_20241212_120000.pkl` (when using `compare_models=True`)

## Why These Are Ignored

1. **Size**: Data and model files can be large (10MB - 1GB+)
2. **Generated**: Can be recreated by running the pipeline
3. **Personal**: May contain user-specific data
4. **Dynamic**: Data updates frequently during development
5. **Models**: Can be retrained anytime

## How to Recreate

If you clone the repository, simply run:

```bash
# Collect fresh data and train a model
python3 main.py
```

Or use the API:

```bash
# Start the API
python3 api.py

# Train a model
curl -X POST "http://localhost:8000/train" \
  -H "Content-Type: application/json" \
  -d '{"player_name": "LeBron James"}'
```

## What Gets Committed

✅ **Committed to git:**
- Source code (`src/`, `main.py`, `api.py`)
- Documentation (`README.md`, API docs)
- Configuration (`requirements.txt`, `pyproject.toml`)
- Tests (`test_*.py`)
- Empty data/models directories (with `.gitkeep` if needed)

❌ **Not committed (ignored):**
- `data/*.json` - Data files
- `models/*.pkl` - Trained models
- `__pycache__/` - Python cache
- `.venv/` - Virtual environment
- IDE files

## Sharing Models

If you need to share trained models:

1. **Upload to cloud storage** (S3, Google Drive, etc.)
2. **Use model registry** (MLflow, Weights & Biases)
3. **Share via API** - Deploy the API and others can train their own
4. **Git LFS** - For version-controlled models (advanced)

## Production Deployment

For production, consider:
- Storing models in S3/GCS/Azure Blob
- Using a model registry
- Versioning models with metadata
- Caching data appropriately

